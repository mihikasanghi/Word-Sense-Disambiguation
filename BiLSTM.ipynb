{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import semcor\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from transformers import BertTokenizer\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('./SemCor/semcor_data.csv')\n",
    "train_data = pd.read_csv('../semcor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def getNewData(data):\n",
    "    new_data = pd.DataFrame(columns=['sentence','target_word', 'sense', 'gloss'])\n",
    "\n",
    "    for i in tqdm(range(0,len(data))):\n",
    "        sentence = data.iloc[i]['sentence']\n",
    "        idx1 = sentence.find('[TGT]')\n",
    "        idx2 = sentence.find('[TGT]', idx1+1)\n",
    "        target_word = sentence[idx1+6:idx2-1]\n",
    "        sentence = sentence.replace('[TGT]', '')\n",
    "        sense_keys = data.iloc[i]['sense_keys']\n",
    "        glosses = data.iloc[i]['glosses']\n",
    "        target = data.iloc[i]['target']\n",
    "        sense_keys = sense_keys.strip('[]')\n",
    "        sense_keys = sense_keys.split(',')\n",
    "        target = target.strip('[]')\n",
    "        target = target.split(',')\n",
    "        glosses = glosses.strip('[]')\n",
    "        glosses = glosses.split(',')\n",
    "        # for every target value add the correspodinign sense key in a new column and also a new column for the gloss\n",
    "        for j in range(0,len(target)):\n",
    "            tgt = int(target[j])\n",
    "            new_row = {'sentence': sentence, 'sense': sense_keys[tgt], 'gloss': glosses[tgt], 'target_word': target_word}\n",
    "            new_data = pd.concat([new_data, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "            new_data['sense'] = new_data['sense'].str.replace('\"', '')\n",
    "            new_data['sense'] = new_data['sense'].str.replace(\"'\", '')\n",
    "    return new_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = getNewData(train_data[:30000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = getNewData(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word_idx = {}\n",
    "idx_to_target = {}\n",
    "sense_labels = []\n",
    "lemma_2_sense = {}\n",
    "for i in range(0,len(train_data)):\n",
    "    sense_label = train_data.iloc[i]['sense']\n",
    "    sense_label = sense_label.replace(' ','')\n",
    "    lemma, pos, wnsn,wnsn2 = sense_label.split('%')[0], int(sense_label.split(\n",
    "        '%')[1].split(':')[0]), sense_label.split('%')[1].split(':')[1],sense_label.split('%')[1].split(':')[2]\n",
    "    new_label = lemma + '%' + str(pos) + '%' + wnsn + '%' + wnsn2\n",
    "    if lemma not in lemma_2_sense:\n",
    "        lemma_2_sense[lemma] = []\n",
    "        target_word_idx[lemma] = len(target_word_idx)\n",
    "        idx_to_target[len(idx_to_target)] = lemma\n",
    "    if sense_label not in lemma_2_sense[lemma]:\n",
    "        lemma_2_sense[lemma].append(sense_label)\n",
    "    # sense_labels.append(new_label)\n",
    "target_word_idx['<unk>'] = len(target_word_idx)\n",
    "idx_to_target[len(idx_to_target)] = '<unk>'\n",
    "lemma_2_sense['<unk>'] = ['<unk>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_2_sense['long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv('./semcor_lstm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max length of the sentence \n",
    "\n",
    "class preProcessDataset():\n",
    "    def __init__(self,data,min_freq):\n",
    "        self.data = data\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = []\n",
    "        self.vocab_sense = []\n",
    "        self.sense2idx = {}\n",
    "        self.idx2sense = {}\n",
    "        self.max_len = 0\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        self.lemma_2_sense = {}\n",
    "        self.wordFreq = {}\n",
    "        self.target2idx = target_word_idx\n",
    "        self.word2idx['<pad>'] = len(self.word2idx)\n",
    "        self.idx2word[len(self.idx2word)] = '<pad>'\n",
    "        self.vocab.append('<pad>')\n",
    "        self.vocab.append('<unk>')\n",
    "        self.word2idx['<unk>'] = len(self.word2idx)\n",
    "        self.idx2word[len(self.idx2word)] = '<unk>'\n",
    "        self.vocab_sense.append('<unk>')\n",
    "        self.sense2idx['<unk>'] = len(self.sense2idx)\n",
    "        self.idx2sense[len(self.idx2sense)] = '<unk>'\n",
    "        \n",
    "\n",
    "\n",
    "        data = self.data\n",
    "        for i in tqdm(range(len(data))):\n",
    "            sentence = data.iloc[i]['sentence']\n",
    "            target_word = data.iloc[i]['sense'].split('%')[0]\n",
    "            target_word = target_word.lower()\n",
    "            target_word = target_word.replace(' ','')\n",
    "            sense_keys = data.iloc[i]['sense']\n",
    "            sense_keys = sense_keys.replace(' ','')\n",
    "\n",
    "            sentence = sentence.split()\n",
    "            # count freq of words\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if word not in self.wordFreq:\n",
    "                    self.wordFreq[word] = 0\n",
    "                self.wordFreq[word] += 1\n",
    "\n",
    "\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                # punctuation marks\n",
    "                if word in ['.',',','?','!',';',':','(',')','[',']','{','}',\"'\",'\"']:\n",
    "                    continue\n",
    "                if self.wordFreq[word] < self.min_freq:\n",
    "                    word = '<unk>'\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = len(self.word2idx)\n",
    "                    self.idx2word[len(self.idx2word)] = word\n",
    "                    self.vocab.append(word)\n",
    "            if len(sentence) > self.max_len:\n",
    "                self.max_len = len(sentence)\n",
    "            if sense_keys not in self.sense2idx:\n",
    "                self.sense2idx[sense_keys] = len(self.sense2idx)\n",
    "                self.idx2sense[len(self.idx2sense)] = sense_keys\n",
    "                self.vocab_sense.append(sense_keys)\n",
    "                \n",
    "\n",
    "class getDataset(Dataset):\n",
    "    def __init__(self, data, word2idx, sense2idx, max_len, target2idx,idx2word,wordFreq,vocab):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        self.sense2idx = sense2idx\n",
    "        self.max_len = max_len\n",
    "        self.target2idx = target2idx\n",
    "        self.idx2word = idx2word\n",
    "        self.wordFreq = wordFreq\n",
    "        self.vocab = vocab\n",
    "        self.input_data = []\n",
    "        self.sense_data = []\n",
    "        self.target2word = []\n",
    "\n",
    "\n",
    "        for i in tqdm(range(len(data))):\n",
    "            sentence = data.iloc[i]['sentence']\n",
    "            sense_keys = data.iloc[i]['sense']\n",
    "            sense_keys = sense_keys.replace(' ','')\n",
    "            target_word = sense_keys.split('%')[0]\n",
    "            target_word = target_word.lower()\n",
    "\n",
    "            target_word = target_word.replace(' ','')\n",
    "            sense_keys = sense_keys.replace(' ','')\n",
    "            sentence = sentence.split()\n",
    "            sentence_idx = []\n",
    "            sense_idx = []\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                # punctuation marks\n",
    "                if word in ['.',',','?','!',';',':','(',')','[',']','{','}',\"'\",'\"']:\n",
    "                    continue\n",
    "                if word not in self.word2idx:\n",
    "                    word = '<unk>'\n",
    "                sentence_idx.append(self.word2idx[word])\n",
    "            while len(sentence_idx) < self.max_len:\n",
    "                sentence_idx.append(self.word2idx['<pad>'])\n",
    "            self.input_data.append(sentence_idx)\n",
    "            # sense_idx.append(self.sense2idx[sense_keys])\n",
    "            if sense_keys not in self.sense2idx:\n",
    "                sense_keys = '<unk>'\n",
    "            self.sense_data.append(self.sense2idx[sense_keys])\n",
    "            if target_word not in self.target2idx:\n",
    "                target_word = '<unk>'\n",
    "            self.target2word.append(self.target2idx[target_word])\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return torch.tensor(self.input_data[idx]),torch.tensor(self.sense_data[idx]),torch.tensor(self.target2word[idx])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preProcessDataset = preProcessDataset(train_data,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = getDataset(train_data,preProcessDataset.word2idx,preProcessDataset.sense2idx,preProcessDataset.max_len,preProcessDataset.target2idx,preProcessDataset.idx2word,preProcessDataset.wordFreq,preProcessDataset.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = getDataset(test_data, preProcessDataset.word2idx, preProcessDataset.sense2idx, preProcessDataset.max_len,\n",
    "                      preProcessDataset.target2idx, preProcessDataset.idx2word, preProcessDataset.wordFreq, preProcessDataset.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataset, valid_dataset = train_test_split(trainData, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(testData, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class biLSTMModel(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,sense_vocab,embedding_dim,dataset):\n",
    "        super(biLSTMModel,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sense_vocab = sense_vocab\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dataset = dataset\n",
    "        self.embedding = nn.Embedding(self.input_size,self.embedding_dim)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim,self.hidden_size,bidirectional=True)\n",
    "        self.linear = nn.Linear(self.hidden_size*2,len(self.sense_vocab))\n",
    "        self.sense2idx = self.dataset.sense2idx\n",
    "        self.idx2word = idx_to_target\n",
    "\n",
    "    def forward(self,x,target_word):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1,0,2)\n",
    "        output,(hidden,cell) = self.lstm(x)\n",
    "        hidden = torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)\n",
    "        out = self.linear(hidden)\n",
    "        for i,target_wo in enumerate(target_word):\n",
    "            target_wo = idx_to_target[target_wo.item()]\n",
    "            target_word_sense = lemma_2_sense[target_wo]\n",
    "            target_word_sense_idx = [self.sense2idx[sense] for sense in target_word_sense]\n",
    "            out[i,target_word_sense_idx] = F.softmax(out[i,target_word_sense_idx],dim=0)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "model = biLSTMModel(len(preProcessDataset.word2idx),128,preProcessDataset.vocab_sense,300,preProcessDataset)\n",
    "model = model.cuda()\n",
    "criterion = CrossEntropyLoss(ignore_index=preProcessDataset.word2idx['<pad>'])\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for i,(sentence,sense,target_word) in enumerate(train_dataloader):\n",
    "        sentence = sentence.cuda()\n",
    "        sense = sense.cuda()\n",
    "        target_word = target_word.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sentence,target_word)\n",
    "        # print(output)\n",
    "        loss = criterion(output,sense)\n",
    "        pred_sense = torch.argmax(output,dim=1)\n",
    "        correct = torch.sum(pred_sense == sense)\n",
    "        total_correct += correct.item()\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    print('Epoch : {}/{} | Loss : {:.4f} | Accuracy : {:.4f}'.format(epoch+1,num_epochs,total_loss/len(train_dataloader),total_correct/len(train_dataset)))\n",
    "\n",
    "    # validation on test dataset \n",
    "    total_correct = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i,(sentence,sense,target_word) in enumerate(valid_dataloader):\n",
    "            sentence = sentence.cuda()\n",
    "            sense = sense.cuda()\n",
    "            target_word = target_word.cuda()\n",
    "            output = model(sentence,target_word)\n",
    "            loss = criterion(output,sense)\n",
    "            pred_sense = torch.argmax(output,dim=1)\n",
    "            correct = torch.sum(pred_sense == sense)\n",
    "            total_correct += correct.item()\n",
    "            total_loss += loss.item()\n",
    "        print('Epoch : {}/{} | Validation Loss : {:.4f} | Validation Accuracy : {:.4f}'.format(epoch+1,num_epochs,total_loss/len(valid_dataloader),total_correct/len(valid_dataset)))\n",
    "\n",
    "torch.save(model.state_dict(),'./ckpts/biLSTM_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct = 0\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (sentence, sense, target_word) in enumerate(test_dataloader):\n",
    "        sentence = sentence.cuda()\n",
    "        sense = sense.cuda()\n",
    "        target_word = target_word.cuda()\n",
    "        output = model(sentence, target_word)\n",
    "        loss = criterion(output, sense)\n",
    "        pred_sense = torch.argmax(output, dim=1)\n",
    "        correct = torch.sum(pred_sense == sense)\n",
    "        total_correct += correct.item()\n",
    "        total_loss += loss.item()\n",
    "    print('| Testing Loss : {:.4f} | Testing Accuracy : {:.4f}'.format(\n",
    "        total_loss/len(test_dataloader), total_correct/len(testData)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
